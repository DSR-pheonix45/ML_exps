# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUDIzM7eXviE-4AB8z5jaR0xTiZlnuwy

EM
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# 1. Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='Target')

# 2. Visualize the data
sns.pairplot(pd.concat([X, y], axis=1), hue="Target", palette="Set1")
plt.suptitle(" Iris Dataset Pairplot", y=1.02)
plt.show()

# 3. Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Fit a Gaussian Mixture Model (GMM) using Expectation-Maximization
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X_train)

# 5. Predict the cluster assignments for the test set
y_pred = gmm.predict(X_test)

# 6. Evaluate the model by comparing with true labels
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 7. Accuracy Score (comparison of predicted cluster labels with true labels)
accuracy = accuracy_score(y_test, y_pred)
print("\n Accuracy Score:", accuracy)

# 8. Visualizing the GMM results
plt.figure(figsize=(6, 6))
plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred, cmap='viridis', marker='o', s=100, edgecolors='black')
plt.title("Expectation-Maximization (EM) Clusters (GMM) on Iris Dataset")
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.grid(True)
plt.show()

# 9. Display the learned GMM parameters (Means and Covariances)
print("\n Learned GMM Parameters:")
print("Means of the components:")
print(gmm.means_)
print("\nCovariances of the components:")
print(gmm.covariances_)

"""Hebbian (and)"""

import numpy as np
import matplotlib.pyplot as plt

# 1. Define the AND input and output
# Inputs: X = [x1, x2], Output: y = x1 AND x2
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data
y = np.array([0, 0, 0, 1])  # Output (AND operation)

# 2. Initialize weights and learning rate
weights = np.zeros(X.shape[1])  # Initialize weights as zeros
learning_rate = 0.1  # Learning rate

# 3. Hebbian learning rule (W = W + Î· * X * y)
def hebbian_learning(X, y, weights, learning_rate, epochs=10):
    for epoch in range(epochs):
        for i in range(len(X)):
            # Update weights based on Hebbian learning rule
            weights += learning_rate * X[i] * y[i]
        print(f"Epoch {epoch + 1}: Weights = {weights}")
    return weights

# 4. Train the model using Hebbian learning
weights = hebbian_learning(X, y, weights, learning_rate, epochs=10)

# 5. Evaluate the trained model (Predict using final weights)
def predict(X, weights):
    return np.where(np.dot(X, weights) > 0, 1, 0)

y_pred = predict(X, weights)

# 6. Display results
print("\n Predicted Outputs:")
print(y_pred)

# 7. Plot decision boundary
x1 = np.linspace(-0.5, 1.5, 100)
x2 = -(weights[0] * x1) / weights[1]

plt.figure(figsize=(6, 6))
plt.plot(x1, x2, label="Decision Boundary")
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', marker='o', s=100, edgecolors='black')
plt.title("Hebbian Learning for AND Operation")
plt.xlabel("x1")
plt.ylabel("x2")
plt.xlim(-0.5, 1.5)
plt.ylim(-0.5, 1.5)
plt.grid(True)
plt.legend()
plt.show()

"""hebbian rule (or)"""

import numpy as np
import matplotlib.pyplot as plt

# 1. Define the OR input and output
# Inputs: X = [x1, x2], Output: y = x1 OR x2
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data
y = np.array([0, 1, 1, 1])  # Output (OR operation)

# 2. Initialize weights and learning rate
weights = np.zeros(X.shape[1])  # Initialize weights as zeros
learning_rate = 0.1  # Learning rate

# 3. Hebbian learning rule (W = W + Î· * X * y)
def hebbian_learning(X, y, weights, learning_rate, epochs=10):
    for epoch in range(epochs):
        for i in range(len(X)):
            # Update weights based on Hebbian learning rule
            weights += learning_rate * X[i] * y[i]
        print(f"Epoch {epoch + 1}: Weights = {weights}")
    return weights

# 4. Train the model using Hebbian learning
weights = hebbian_learning(X, y, weights, learning_rate, epochs=10)

# 5. Evaluate the trained model (Predict using final weights)
def predict(X, weights):
    return np.where(np.dot(X, weights) > 0, 1, 0)

y_pred = predict(X, weights)

# 6. Display results
print("\n Predicted Outputs:")
print(y_pred)

# 7. Plot decision boundary
x1 = np.linspace(-0.5, 1.5, 100)
x2 = -(weights[0] * x1) / weights[1]

plt.figure(figsize=(6, 6))
plt.plot(x1, x2, label="Decision Boundary")
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', marker='o', s=100, edgecolors='black')
plt.title("Hebbian Learning for OR Operation")
plt.xlabel("x1")
plt.ylabel("x2")
plt.xlim(-0.5, 1.5)
plt.ylim(-0.5, 1.5)
plt.grid(True)
plt.legend()
plt.show()

"""linear REGS"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the diabetes dataset
diabetes = load_diabetes()

# Convert to a pandas DataFrame
df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
df['target'] = diabetes.target

# Display first 5 rows
print(df.head())

# Basic stats
print("Statistical Summary:")
print(df.describe())

# Correlation matrix
print("\nCorrelation Matrix:")
print(df.corr())

# Visualize correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# Use 'bmi' to predict target
X = df[['bmi']]
y = df['target']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Regression Line Plot
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Prediction')
plt.xlabel('BMI')
plt.ylabel('Diabetes Progression')
plt.title('Linear Regression: BMI vs Target')
plt.legend()
plt.show()

print("Intercept:", model.intercept_)
print("Coefficient (Slope):", model.coef_[0])

# Mean Squared Error
print("MSE:", mean_squared_error(y_test, y_pred))

# RÂ² Score
print("RÂ² Score:", r2_score(y_test, y_pred))

"""logistic reg (multi way)"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the diabetes dataset
diabetes = load_diabetes()

# Convert to a pandas DataFrame
df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
df['target'] = diabetes.target

# Display first 5 rows
print(df.head())

# Basic stats
print("Statistical Summary:")
print(df.describe())

# Correlation matrix
print("\nCorrelation Matrix:")
print(df.corr())

# Visualize correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# Use 'bmi' to predict target
X = df[['bmi']]
y = df['target']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Regression Line Plot
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Prediction')
plt.xlabel('BMI')
plt.ylabel('Diabetes Progression')
plt.title('Linear Regression: BMI vs Target')
plt.legend()
plt.show()

print("Intercept:", model.intercept_)
print("Coefficient (Slope):", model.coef_[0])

# Mean Squared Error
print("MSE:", mean_squared_error(y_test, y_pred))

# RÂ² Score
print("RÂ² Score:", r2_score(y_test, y_pred))

"""logistic (2 way)"""

import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name='Target')

# 2. Display class labels
print(" Target Classes:")
print(data.target_names)

# 3. Basic data insights
print("\n Dataset Info:")
print(X.info())
print("\n Dataset Description:")
print(X.describe())

# 4. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Train Logistic Regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# 6. Predict
y_pred = model.predict(X_test)

# 7. Evaluate
print("\n Accuracy Score:", accuracy_score(y_test, y_pred))
print("\n Classification Report:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

# 8. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# 9. Coefficients
coeff_df = pd.DataFrame(model.coef_[0], X.columns, columns=["Coefficient"])
print("\n Feature Coefficients:")
print(coeff_df.sort_values(by="Coefficient", key=abs, ascending=False))

"""Mp Neuron (and)"""

import numpy as np

# 1. Define the AND truth table inputs and outputs
# Inputs: x1, x2 â†’ Output: y = x1 AND x2
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input combinations
y = np.array([0, 0, 0, 1])                     # AND operation outputs

# 2. Define the McCulloch-Pitts model
def mcp_neuron(X, weights, threshold):
    # Compute weighted sum
    net_input = np.dot(X, weights)
    # Apply step activation function
    output = np.where(net_input >= threshold, 1, 0)
    return output

# 3. Set weights and threshold for AND operation
weights = np.array([1, 1])  # weights for x1 and x2
threshold = 2               # threshold for AND logic

# 4. Get predictions
predictions = mcp_neuron(X, weights, threshold)

# 5. Display Results
print(" McCulloch-Pitts Model for AND Gate\n")
print("Inputs\tOutput")
for inp, out in zip(X, predictions):
    print(f"{inp}\t  {out}")

# 6. Evaluation
print("\n Matches Truth Table:", np.array_equal(predictions, y))

"""Mp Neuron (OR)"""

import numpy as np

# 1. Define the OR truth table inputs and outputs
# Inputs: x1, x2 â†’ Output: y = x1 OR x2
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input combinations
y = np.array([0, 1, 1, 1])                     # OR operation outputs

# 2. Define the McCulloch-Pitts model
def mcp_neuron(X, weights, threshold):
    # Net input = x1*w1 + x2*w2
    net_input = np.dot(X, weights)
    # Apply step activation function
    output = np.where(net_input >= threshold, 1, 0)
    return output

# 3. Set weights and threshold for OR operation
weights = np.array([1, 1])  # weights for x1 and x2
threshold = 1               # threshold for OR logic

# 4. Get predictions
predictions = mcp_neuron(X, weights, threshold)

# 5. Display Results
print(" McCulloch-Pitts Model for OR Gate\n")
print("Inputs\tOutput")
for inp, out in zip(X, predictions):
    print(f"{inp}\t  {out}")

# 6. Evaluation
print("\n Matches Truth Table:", np.array_equal(predictions, y))

"""MP Xor"""

import numpy as np

# Input combinations for XOR
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_expected = np.array([0, 1, 1, 0])

# Define MCP neuron
def mcp_neuron(inputs, weights, threshold):
    net_input = np.dot(inputs, weights)
    return np.where(net_input >= threshold, 1, 0)

# OR gate using MCP: weights = [1, 1], threshold = 1
def or_gate(inputs):
    return mcp_neuron(inputs, weights=np.array([1, 1]), threshold=1)

# AND gate using MCP: weights = [1, 1], threshold = 2
def and_gate(inputs):
    return mcp_neuron(inputs, weights=np.array([1, 1]), threshold=2)

# NOT gate for a single input: weight = -1, threshold = 0
def not_gate(input_val):
    return np.where(input_val >= 1, 0, 1)

# XOR = (x1 OR x2) AND NOT(x1 AND x2)
def xor_gate(X):
    or_result = or_gate(X)
    and_result = and_gate(X)
    not_and_result = not_gate(and_result)
    # Combine OR and NOT(AND)
    combined = np.column_stack((or_result, not_and_result))
    return and_gate(combined)

# Predictions
y_pred = xor_gate(X)

# Display Results
print("McCulloch-Pitts Model for XOR Gate\n")
print("Inputs\tOutput")
for inp, out in zip(X, y_pred):
    print(f"{inp}\t  {out}")

# Evaluation
print("\nMatches XOR Truth Table:", np.array_equal(y_pred, y_expected))

"""Multilinear regression"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# 1. Load dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name="Target")

# 2. Dataset Summary
print(" Dataset Head:")
print(X.head())

print("\n Dataset Info:")
print(X.info())

print("\n Dataset Description:")
print(X.describe())

# 3. Correlation matrix
print("\n Correlation Matrix:")
correlation_matrix = X.corr()
print(correlation_matrix)

# Optional: visualize correlations
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# 4. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# 6. Predictions
y_pred = model.predict(X_test)

# 7. Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("\nðŸ“‹ Model Evaluation:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"RÂ² Score: {r2:.4f}")

# 8. Coefficient interpretation
coeff_df = pd.DataFrame(model.coef_, X.columns, columns=["Coefficient"])
print("\n Feature Coefficients:")
print(coeff_df)

"""PCA"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. Load the dataset
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

# 2. Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Apply PCA to reduce to 2 components
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 4. Create a DataFrame for visualization
df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
df_pca['Target'] = y

# 5. Plot the PCA components
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='Target', palette='Set1', s=100)
plt.title('PCA of Iris Dataset')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance)')
plt.grid(True)
plt.show()

# 6. Print explained variance
print("Explained Variance Ratio of Components:")
for i, var in enumerate(pca.explained_variance_ratio_):
    print(f"PC{i+1}: {var:.4f}")

"""Perceptron (And)"""

import numpy as np

# 1. Define the AND gate dataset
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y = np.array([0, 0, 0, 1])  # Output of AND gate

# 2. Add bias term to input
X_bias = np.c_[np.ones((X.shape[0], 1)), X]  # Shape: (4, 3)

# 3. Initialize weights and parameters
weights = np.zeros(X_bias.shape[1])  # [w0, w1, w2]
learning_rate = 0.1
epochs = 10

# 4. Train using Perceptron Learning Rule
print("Training Perceptron for AND Gate")
for epoch in range(epochs):
    total_error = 0
    for xi, target in zip(X_bias, y):
        output = 1 if np.dot(weights, xi) >= 0 else 0
        error = target - output
        weights += learning_rate * error * xi
        total_error += abs(error)
    print(f"Epoch {epoch + 1} | Total Error: {total_error}")
    if total_error == 0:
        break

# 5. Final weights
print("\nFinal Weights:", weights)

# 6. Testing
print("\nTesting Perceptron on AND Gate")
print("Inputs\tOutput")
for xi, target in zip(X, y):
    xi_with_bias = np.insert(xi, 0, 1)  # add bias
    output = 1 if np.dot(weights, xi_with_bias) >= 0 else 0
    print(f"{xi}\t  {output}")

"""perceptron (OR)"""

import numpy as np

# 1. Define the OR dataset
# Inputs: x1, x2; Outputs: x1 OR x2
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y = np.array([0, 1, 1, 1])  # Expected output for OR gate

# 2. Add bias term (x0 = 1)
X_bias = np.c_[np.ones((X.shape[0], 1)), X]

# 3. Initialize weights and parameters
weights = np.zeros(X_bias.shape[1])  # [w0, w1, w2]
learning_rate = 0.1
epochs = 10

# 4. Training using Perceptron Learning Rule
print("Training Perceptron for OR Gate")
for epoch in range(epochs):
    total_error = 0
    for xi, target in zip(X_bias, y):
        output = 1 if np.dot(weights, xi) >= 0 else 0
        error = target - output
        weights += learning_rate * error * xi
        total_error += abs(error)
    print(f"Epoch {epoch+1} | Total Error: {total_error}")
    if total_error == 0:
        break

# 5. Final Weights
print("\nFinal Weights:", weights)

# 6. Testing
print("\nTesting Perceptron on OR Gate")
print("Inputs\tOutput")
for xi, target in zip(X, y):
    xi_with_bias = np.insert(xi, 0, 1)  # add bias
    output = 1 if np.dot(weights, xi_with_bias) >= 0 else 0
    print(f"{xi}\t  {output}")

"""perceptron (Xor)"""

import numpy as np

# Define input and expected output for XOR gate
inputs = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]])

# Expected output for XOR operation
output = np.array([0, 1, 1, 0])

# Initialize weights and bias randomly
weights = np.random.rand(2)  # Two inputs, so two weights
bias = np.random.rand(1)

# Learning rate
learning_rate = 0.1

# Step activation function
def step_function(x):
    return 1 if x >= 0 else 0

# Training the Perceptron
def perceptron_train(inputs, output, weights, bias, learning_rate, epochs=10):
    for epoch in range(epochs):
        total_error = 0
        for i in range(len(inputs)):
            # Calculate the weighted sum of inputs
            weighted_sum = np.dot(inputs[i], weights) + bias
            # Get the predicted output
            prediction = step_function(weighted_sum)
            # Calculate error
            error = output[i] - prediction
            total_error += abs(error)
            # Update weights and bias
            weights += learning_rate * error * inputs[i]
            bias += learning_rate * error
        print(f"Epoch {epoch+1}, Total Error: {total_error}")
        if total_error == 0:
            print("Training complete with no errors.")
            break
    return weights, bias

# Train the perceptron
weights, bias = perceptron_train(inputs, output, weights, bias, learning_rate)

# Test the perceptron
def test_perceptron(inputs, weights, bias):
    for i in range(len(inputs)):
        weighted_sum = np.dot(inputs[i], weights) + bias
        prediction = step_function(weighted_sum)
        print(f"Input: {inputs[i]}, Predicted Output: {prediction}")

# Test the trained perceptron
test_perceptron(inputs, weights, bias)

"""SVM"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# 1. Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='Target')

# 2. Visualize the data
sns.pairplot(pd.concat([X, y], axis=1), hue="Target", palette="Set1")
plt.suptitle("ðŸ“Š Iris Dataset Pairplot", y=1.02)
plt.show()

# 3. Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Train Support Vector Machine (SVM) model
model = SVC(kernel='linear', random_state=42)
model.fit(X_train, y_train)

# 5. Predict
y_pred = model.predict(X_test)

# 6. Evaluation
print("\nâœ… Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nðŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 7. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Support Vector Machine (SVM)")
plt.show()

# 8. Support Vectors
print("\nðŸ“Œ Number of Support Vectors for each class:")
print(model.n_support_)